{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subjective Quiz | Big Data on Spark\n",
    "\n",
    "Dataset: https://archive.ics.uci.edu/ml/datasets/Wholesale+customers#\n",
    "\n",
    "The dataset refers to clients of a wholesale distributor. It includes the annual spending in monetary units (m.u.) on diverse product categories\n",
    "\n",
    "Variables:\n",
    "\n",
    "1.  FRESH: annual spending (m.u.) on fresh products (Continuous);\n",
    "2.  MILK: annual spending (m.u.) on milk products (Continuous);\n",
    "3.  GROCERY: annual spending (m.u.)on grocery products (Continuous);\n",
    "4.  FROZEN: annual spending (m.u.)on frozen products (Continuous)\n",
    "5.  DETERGENTS_PAPER: annual spending (m.u.) on detergents and paper products (Continuous)\n",
    "6.  DELICATESSEN: annual spending (m.u.)on and delicatessen products (Continuous);\n",
    "7.  CHANNEL: customers Channel - Horeca (Hotel/Restaurant/Cafe) or Retail channel (Nominal)\n",
    "8.  REGION: customers Region Lisbon, Oporto or Other (Nominal)\n",
    "\n",
    "Goal:\n",
    "Learn DataFrame Operations in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext,SQLContext,SparkConf,StorageLevel\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkContext and SparkConfiguration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=SubQuizMP, master=local[3]) created by __init__ at <ipython-input-4-06bfe0a9bdbb>:3 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-06bfe0a9bdbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# setMaster(\"local[3]\") says it's local driver with 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msparkconf\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SubQuizMP\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetMaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local[3]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msc\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msparkconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    297\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 299\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=SubQuizMP, master=local[3]) created by __init__ at <ipython-input-4-06bfe0a9bdbb>:3 "
     ]
    }
   ],
   "source": [
    "# setMaster(\"local[3]\") says it's local driver with 3 \n",
    "sparkconf= SparkConf().setAppName(\"SubQuizMP\").setMaster(\"local[3]\")\n",
    "sc= SparkContext(conf=sparkconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read the csv file as a dataframe. And, not as RDD. See the schema of the DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of the df:\n",
      "[('Channel', 'int'), ('Region', 'int'), ('Fresh', 'int'), ('Milk', 'int'), ('Grocery', 'int'), ('Frozen', 'int'), ('Detergents_Paper', 'int'), ('Delicassen', 'int')]\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('Wholesalecustomersdata.csv')\n",
    "\n",
    "print(\"Schema of the df:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Use select to view a single column or a set of chosen columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|Channel|\n",
      "+-------+\n",
      "|      2|\n",
      "|      2|\n",
      "|      2|\n",
      "|      1|\n",
      "|      2|\n",
      "|      2|\n",
      "|      2|\n",
      "|      2|\n",
      "|      1|\n",
      "|      2|\n",
      "|      2|\n",
      "|      2|\n",
      "|      2|\n",
      "|      2|\n",
      "|      2|\n",
      "|      1|\n",
      "|      2|\n",
      "|      1|\n",
      "|      2|\n",
      "|      1|\n",
      "+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df['Channel']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+-------+------+\n",
      "|Region|Fresh| Milk|Grocery|Frozen|\n",
      "+------+-----+-----+-------+------+\n",
      "|     3|12669| 9656|   7561|   214|\n",
      "|     3| 7057| 9810|   9568|  1762|\n",
      "|     3| 6353| 8808|   7684|  2405|\n",
      "|     3|13265| 1196|   4221|  6404|\n",
      "|     3|22615| 5410|   7198|  3915|\n",
      "|     3| 9413| 8259|   5126|   666|\n",
      "|     3|12126| 3199|   6975|   480|\n",
      "|     3| 7579| 4956|   9426|  1669|\n",
      "|     3| 5963| 3648|   6192|   425|\n",
      "|     3| 6006|11093|  18881|  1159|\n",
      "|     3| 3366| 5403|  12974|  4400|\n",
      "|     3|13146| 1124|   4523|  1420|\n",
      "|     3|31714|12319|  11757|   287|\n",
      "|     3|21217| 6208|  14982|  3095|\n",
      "|     3|24653| 9465|  12091|   294|\n",
      "|     3|10253| 1114|   3821|   397|\n",
      "|     3| 1020| 8816|  12121|   134|\n",
      "|     3| 5876| 6157|   2933|   839|\n",
      "|     3|18601| 6327|  10099|  2205|\n",
      "|     3| 7780| 2495|   9464|   669|\n",
      "+------+-----+-----+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df['Region'],df['Fresh'],df['Milk'],df['Grocery'],df['Frozen']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Use filter to see records with fresh sales more than 50000 only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+-----+-------+------+----------------+----------+\n",
      "|Channel|Region| Fresh| Milk|Grocery|Frozen|Detergents_Paper|Delicassen|\n",
      "+-------+------+------+-----+-------+------+----------------+----------+\n",
      "|      1|     3| 56159|  555|    902| 10002|             212|      2916|\n",
      "|      1|     3| 56082| 3504|   8906| 18028|            1480|      2498|\n",
      "|      1|     3| 76237| 3473|   7102| 16538|             778|       918|\n",
      "|      1|     3|112151|29627|  18148| 16745|            4948|      8550|\n",
      "|      1|     1| 56083| 4563|   2124|  6422|             730|      3321|\n",
      "|      1|     1| 53205| 4959|   7336|  3012|             967|       818|\n",
      "|      1|     3| 68951| 4411|  12609|  8692|             751|      2406|\n",
      "+-------+------+------+-----+-------+------+----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.Fresh > 50000).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create aggregates on channels and regions variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Channel=1, count(1)=298), Row(Channel=2, count(1)=142)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf = df.groupBy(df.Channel)\n",
    "df_Channel = sorted(gdf.agg({\"*\": \"count\"}).collect())\n",
    "df_Channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Region=1, count(1)=77),\n",
       " Row(Region=2, count(1)=47),\n",
       " Row(Region=3, count(1)=316)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf = df.groupBy(df.Region)\n",
    "df_Region = sorted(gdf.agg({\"*\": \"count\"}).collect())\n",
    "df_Region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Use describe to see summary statistics on dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+-----------------+-----------------+------------------+------------------+\n",
      "|summary|           Channel|            Region|             Fresh|              Milk|          Grocery|           Frozen|  Detergents_Paper|        Delicassen|\n",
      "+-------+------------------+------------------+------------------+------------------+-----------------+-----------------+------------------+------------------+\n",
      "|  count|               440|               440|               440|               440|              440|              440|               440|               440|\n",
      "|   mean|1.3227272727272728| 2.543181818181818|12000.297727272728| 5796.265909090909|7951.277272727273|3071.931818181818|2881.4931818181817|1524.8704545454545|\n",
      "| stddev|0.4680515694791137|0.7742724492301002|12647.328865076885|7380.3771745708445|9503.162828994346|4854.673332592367| 4767.854447904201|2820.1059373693965|\n",
      "|    min|                 1|                 1|                 3|                55|                3|               25|                 3|                 3|\n",
      "|    max|                 2|                 3|            112151|             73498|            92780|            60869|             40827|             47943|\n",
      "+-------+------------------+------------------+------------------+------------------+-----------------+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Change datatype of Channels to Strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = df.withColumn('Channel',df.Channel.cast('String'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Channel', 'string'),\n",
       " ('Region', 'int'),\n",
       " ('Fresh', 'int'),\n",
       " ('Milk', 'int'),\n",
       " ('Grocery', 'int'),\n",
       " ('Frozen', 'int'),\n",
       " ('Detergents_Paper', 'int'),\n",
       " ('Delicassen', 'int')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|Channel|\n",
      "+-------+\n",
      "|      2|\n",
      "|      2|\n",
      "|      2|\n",
      "|      1|\n",
      "|      2|\n",
      "|      2|\n",
      "|      2|\n",
      "|      2|\n",
      "|      1|\n",
      "|      2|\n",
      "|      2|\n",
      "|      2|\n",
      "|      2|\n",
      "|      2|\n",
      "|      2|\n",
      "|      1|\n",
      "|      2|\n",
      "|      1|\n",
      "|      2|\n",
      "|      1|\n",
      "+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select(df1['Channel']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
